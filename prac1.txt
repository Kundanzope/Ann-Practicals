import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
x = np.linspace(-10, 10, 1000)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def tanh(x):
    return np.tanh(x)
def relu(x):
    return np.maximum(0, x)
def leaky_relu(x, alpha=0.1):
    return np.where(x > 0, x, alpha * x)
def swish(x):
    return x * sigmoid(x)
plt.figure(figsize=(10, 8))
plt.plot(x, sigmoid(x), label="Sigmoid")
plt.plot(x, tanh(x), label="Tanh")
plt.plot(x, relu(x), label="ReLU")
plt.plot(x, leaky_relu(x), label="Leaky ReLU")
plt.plot(x, swish(x), label="Swish")
plt.title("Common Activation Functions in Neural Networks")
plt.xlabel("Input")
plt.ylabel("Output")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
